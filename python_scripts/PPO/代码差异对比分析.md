# 代码差异对比分析

## 一、文件对应关系

**旧版本（HPPO混合策略）**：
- `PPO_episoid_1.py` (旧) - 使用HPPO混合动作空间
- `hppo_01.py` - HPPO实现（离散+连续混合）

**新版本（独立多智能体）**：
- `PPO_episoid_1.py` (新) - 使用两个独立PPO（shoulder和arm）
- `PPO_PPOnet.py` - 单个PPO实现

---

## 二、核心架构差异

### 2.1 网络架构

#### 旧版本（HPPO）
```python
# hppo_01.py - MultiDiscreteActorCritic
class MultiDiscreteActorCritic:
    - 离散动作头：discrete_head (输出 num_servos*2 logits)
    - 连续动作头：continuous (输出 num_servos 个连续值)
    - 共享特征层：fc4 (300 -> 200)
    - 图神经网络：简化为全连接层 (fc_graph: 20 -> 100)
    - 输出：discrete_dist, continuous_dist, value
```

**特点**：
- ✅ 单一网络同时输出离散和连续动作
- ✅ 共享特征提取层
- ✅ 图神经网络简化（不使用GraphSAGE/GAT等）

#### 新版本（独立PPO）
```python
# PPO_PPOnet.py - ActorCritic
class ActorCritic:
    - Actor头：actor_mu (输出 act_dim 个连续值)
    - 标准差：actor_log_sigma (可学习参数)
    - 共享特征层：fc4 (300 -> 200)
    - 图神经网络：完整的GNN结构
        * GraphSAGE (1 -> 1000)
        * GATConv (1000 -> 1000)
        * GraphSAGE (1000 -> 1000)
        * GATConv (1000 -> 1000)
        * GCNConv (1000 -> 1000)
    - 输出：dist (Normal分布), value
```

**特点**：
- ✅ 每个智能体独立网络
- ✅ 完整的图神经网络结构（5层GNN）
- ✅ 使用tanh squashing修正log_prob

---

### 2.2 动作空间设计

#### 旧版本（HPPO）
```python
# 离散动作：控制是否执行动作（开关）
discrete_action = [d0, d1]  # d0控制shoulder，d1控制arm
# 连续动作：实际动作值
continuous_action = [shoulder_value, arm_value]

# 动作掩码机制
masked_shoulder = prev_shoulder_action if int(d0) == 0 else cur_shoulder
masked_arm = prev_arm_action if int(d1) == 0 else cur_arm
```

**特点**：
- ✅ 离散动作作为"开关"，控制是否更新动作
- ✅ 如果离散=0，保持上一时刻动作
- ✅ 混合动作空间（离散+连续）

#### 新版本（独立PPO）
```python
# 每个智能体独立选择动作
action_shoulder = ppo_shoulder.choose_action(...)  # 直接输出连续值
action_arm = ppo_arm.choose_action(...)  # 直接输出连续值

# 直接使用，无需掩码
action_shoulder_t = np.clip(action_shoulder_t, -0.5, 0.5)
action_arm_t = np.clip(action_arm_t, -0.5, 0.5)
```

**特点**：
- ✅ 每个智能体独立选择动作
- ✅ 直接使用动作值，无需掩码
- ✅ 动作范围限制在[-0.5, 0.5]

---

## 三、训练机制差异

### 3.1 数据存储

#### 旧版本（HPPO）
```python
# hppo_01.py
def store_transition(self, state, discrete_action, continuous_action, 
                     reward, next_state, done, value,
                     discrete_log_prob, continuous_log_prob):
    self.states.append(state)
    self.discrete_actions.append(discrete_action)
    self.continuous_actions.append(continuous_action)
    self.discrete_log_probs.append(discrete_log_prob)
    self.continuous_log_probs.append(continuous_log_prob)
    # ... 其他字段
```

**特点**：
- ✅ 同时存储离散和连续动作
- ✅ 分别存储离散和连续的对数概率
- ✅ 单一存储接口

#### 新版本（独立PPO）
```python
# PPO_PPOnet.py
def store_transition_catch(self, state, action, reward, next_state, 
                          done, value, log_prob, action_raw=None):
    self.states.append(state)
    self.actions.append(action)  # tanh后的动作
    self.actions_raw.append(action_raw)  # 原始动作（用于log_prob计算）
    self.log_probs.append(log_prob)
    # ... 其他字段
```

**特点**：
- ✅ 每个智能体独立存储
- ✅ 同时存储tanh后动作和原始动作
- ✅ 需要action_raw来正确计算log_prob（tanh squashing修正）

---

### 3.2 学习更新机制

#### 旧版本（HPPO）
```python
# hppo_01.py - learn()
def learn(self):
    # 1. 分别计算离散和连续的ratio
    ratios_c = torch.exp(new_continuous_log_probs - batch_continuous_log_probs)
    ratios_d = torch.exp(new_discrete_log_probs - batch_discrete_log_probs)
    
    # 2. 分别计算损失
    discrete_loss = -torch.min(surr1_d, surr2_d).mean(dim=1).mean()
    continuous_loss = -torch.min(surr1_c, surr2_c).mean(dim=1).mean()
    
    # 3. 使用三个独立的优化器
    self.optimizer_d.zero_grad()  # 离散网络
    loss_d.backward(retain_graph=True)
    self.optimizer_d.step()
    
    self.optimizer_c.zero_grad()  # 连续网络
    loss_c.backward(retain_graph=True)
    self.optimizer_c.step()
    
    self.optimizer_v.zero_grad()  # Critic网络
    value_loss.backward()
    self.optimizer_v.step()
    
    return loss_discrete, loss_continuous
```

**特点**：
- ✅ 三个独立优化器（离散、连续、Critic）
- ✅ 使用retain_graph=True允许多次反向传播
- ✅ 分别更新离散和连续网络
- ✅ 更新轮数：policy_update_epochs = 10

#### 新版本（独立PPO）
```python
# PPO_PPOnet.py - learn()
def learn(self):
    # 1. 计算优势函数
    advantages, returns = self.calculate_advantages()
    
    # 2. 批量处理
    for _ in range(self.update_epochs):  # update_epochs = 4
        for start_idx in range(0, len(batch_states), self.batch_size):
            # 3. 计算新策略的log_prob（使用action_raw）
            new_log_prob_raw = dists[i].log_prob(action_raw_i)
            tanh_correction = torch.log(1 - action_tanh_i.pow(2) + 1e-6)
            new_log_prob = (new_log_prob_raw - tanh_correction).sum(dim=-1)
            
            # 4. 计算ratio和损失
            ratio = torch.exp(new_log_prob - batch_log_probs_curr[i])
            policy_loss += -torch.min(surr1, surr2)
            
    # 5. 单一优化器更新
    self.optimizer.zero_grad()
    loss.backward()
    torch.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)
    self.optimizer.step()
    
    return total_loss / self.update_epochs
```

**特点**：
- ✅ 单一优化器更新整个网络
- ✅ 使用tanh squashing修正log_prob
- ✅ 更新轮数：update_epochs = 4（更少）
- ✅ 批处理：batch_size = 64

---

### 3.3 超参数差异

| 参数 | 旧版本（HPPO） | 新版本（独立PPO） |
|------|--------------|------------------|
| **学习率** | 2e-4 | 3e-4 |
| **学习率衰减** | 0.995 | 0.999 |
| **更新轮数** | 10 | 4 |
| **批大小** | 32 (最小) | 64 |
| **clip_ratio** | 0.2 | 0.1 |
| **value_coef** | 0.5 | 0.5 |
| **entropy_coef** | 0.01 | 0.1 |
| **gamma** | 0.99 | 0.99 |
| **gae_lambda** | 0.95 | 0.95 |
| **max_grad_norm** | 0.5 | 1.0 |

**关键差异**：
- ✅ HPPO使用更小的entropy_coef（0.01 vs 0.1），探索更保守
- ✅ HPPO更新轮数更多（10 vs 4），可能过度拟合
- ✅ 独立PPO使用更大的entropy_coef，鼓励探索
- ✅ 独立PPO使用更大的max_grad_norm（1.0 vs 0.5）

---

## 四、环境交互差异

### 4.1 动作执行

#### 旧版本（HPPO）
```python
# PPO_episoid_1.py (旧)
dict = hppo_agent.choose_action(episode_num=i, obs=obs, x_graph=robot_state)
d_action = dict['discrete_action']
action_shoulder = dict['continuous_action'][0]
action_arm = dict['continuous_action'][1]

# 动作掩码
d0 = float(d_action[0])
d1 = float(d_action[1])
masked_shoulder = prev_shoulder_action if int(d0) == 0 else cur_shoulder
masked_arm = prev_arm_action if int(d1) == 0 else cur_arm

# 执行掩码后的动作
env.step(robot_state, masked_shoulder, masked_arm, ...)
```

**特点**：
- ✅ 需要维护prev_shoulder_action和prev_arm_action
- ✅ 根据离散动作决定是否更新动作
- ✅ 动作可能保持不变（如果离散=0）

#### 新版本（独立PPO）
```python
# PPO_episoid_1.py (新)
action_shoulder, log_prob_shoulder, value_shoulder, action_raw_shoulder = \
    ppo_shoulder.choose_action(episode_num=i, obs=obs, x_graph=robot_state)
action_arm, log_prob_arm, value_arm, action_raw_arm = \
    ppo_arm.choose_action(episode_num=i, obs=obs, x_graph=robot_state)

# 直接使用动作
action_shoulder_t = float(action_shoulder[0]) if hasattr(...) else float(action_shoulder)
action_arm_t = float(action_arm[0]) if hasattr(...) else float(action_arm)
action_shoulder_t = np.clip(action_shoulder_t, -0.5, 0.5)
action_arm_t = np.clip(action_arm_t, -0.5, 0.5)

# 直接执行
env.step(robot_state, action_shoulder_t, action_arm_t, ...)
```

**特点**：
- ✅ 无需维护上一时刻动作
- ✅ 每个智能体独立选择动作
- ✅ 动作直接执行，无需掩码

---

### 4.2 数据存储条件

#### 旧版本（HPPO）
```python
# PPO_episoid_1.py (旧)
if good == 1:  # 只有good=1时才存储
    hppo_agent.store_transition(
        state=[obs_img, robot_state, robot_state],
        discrete_action=d_action,
        continuous_action=dict['continuous_action'],
        reward=reward,
        next_state=[next_obs_img, next_state, next_state],
        done=done,
        value=value,
        discrete_log_prob=dict['discrete_log_prob'],
        continuous_log_prob=dict['continuous_log_prob']
    )
```

**特点**：
- ✅ 使用good标志过滤数据
- ✅ 只存储"好"的样本

#### 新版本（独立PPO）
```python
# PPO_episoid_1.py (新)
# 无条件存储所有步的数据
ppo_shoulder.store_transition_catch(
    state=[obs_img, robot_state, robot_state],
    action=action_shoulder,
    reward=reward,
    next_state=[next_obs_img, next_state, next_state],
    done=done,
    value=value_shoulder,
    log_prob=log_prob_shoulder,
    action_raw=action_raw_shoulder
)
ppo_arm.store_transition_catch(...)  # 同样存储
```

**特点**：
- ✅ 无条件存储所有步的数据
- ✅ 符合PPO的on-policy特性
- ✅ 避免采样偏置

---

## 五、模型保存和加载

### 5.1 模型保存格式

#### 旧版本（HPPO）
```python
# PPO_episoid_1.py (旧)
checkpoint = {
    'policy': hppo_agent.policy.state_dict(),
    'optimizer_hppo': hppo_agent.optimizer.state_dict(),
    'episode': i
}
```

**特点**：
- ✅ 单一policy网络
- ✅ 单一optimizer（虽然内部有三个优化器，但只保存主优化器）

#### 新版本（独立PPO）
```python
# PPO_episoid_1.py (新)
base_checkpoint_data = {
    'policy_shoulder': ppo_shoulder.policy.state_dict(),
    'optimizer_shoulder': ppo_shoulder.optimizer.state_dict(),
    'policy_arm': ppo_arm.policy.state_dict(),
    'optimizer_arm': ppo_arm.optimizer.state_dict(),
    'episode': i
}
```

**特点**：
- ✅ 分别保存两个policy网络
- ✅ 分别保存两个optimizer
- ✅ 使用ModelRanking管理最佳模型

---

### 5.2 模型加载

#### 旧版本（HPPO）
```python
# PPO_episoid_1.py (旧)
if isinstance(checkpoint, dict) and 'policy' in checkpoint:
    hppo_agent.policy.load_state_dict(checkpoint['policy'])
    if 'optimizer_hppo' in checkpoint and hppo_agent.optimizer:
        hppo_agent.optimizer.load_state_dict(checkpoint['optimizer_hppo'])
```

**特点**：
- ✅ 加载单一policy
- ✅ 加载单一optimizer

#### 新版本（独立PPO）
```python
# PPO_episoid_1.py (新)
if isinstance(checkpoint, dict) and 'policy_shoulder' in checkpoint:
    ppo_shoulder.policy.load_state_dict(checkpoint['policy_shoulder'])
    ppo_arm.policy.load_state_dict(checkpoint['policy_arm'])
    if 'optimizer_shoulder' in checkpoint and ppo_shoulder.optimizer:
        ppo_shoulder.optimizer.load_state_dict(checkpoint['optimizer_shoulder'])
    if 'optimizer_arm' in checkpoint and ppo_arm.optimizer:
        ppo_arm.optimizer.load_state_dict(checkpoint['optimizer_arm'])
```

**特点**：
- ✅ 分别加载两个policy
- ✅ 分别加载两个optimizer

---

## 六、测试和评估机制

### 6.1 测试频率

#### 旧版本（HPPO）
```python
# PPO_episoid_1.py (旧)
# 没有测试机制
# 只在goal=1时保存模型
if goal == 1:
    save_path = path_list['model_path_catch_PPO'] + '/ppo_model_%s.ckpt' % i
    torch.save(checkpoint, save_path)

# 每100步保存一次模型
if i % 100 == 0 and i != 0:
    save_path = path_list['model_path_catch_PPO'] + '/ppo_model_%s.ckpt' % i
    torch.save(checkpoint, save_path)
```

**特点**：
- ❌ 没有测试机制
- ✅ 只在goal=1或每100步保存模型
- ❌ 没有模型评估和排名

#### 新版本（独立PPO）
```python
# PPO_episoid_1.py (新)
CHECKPOINT_INTERVAL = 500
NUM_TEST_EPISODES = 100

if is_checkpoint_interval:  # 每500个episode
    # 1. 切换到评估模式
    ppo_shoulder.policy.eval()
    ppo_arm.policy.eval()
    
    # 2. 运行100轮测试
    while valid_test_cnt < NUM_TEST_EPISODES:
        # 测试逻辑...
        test_success_rate = successful_test_episodes
    
    # 3. 使用ModelRanking管理最佳模型
    model_ranking.add_and_manage(
        new_score=test_success_rate,
        new_checkpoint=base_checkpoint_data,
        episode_id=i,
        base_dir=path_list['model_path_catch_PPO']
    )
```

**特点**：
- ✅ 每500个episode进行测试
- ✅ 测试100轮计算成功率
- ✅ 使用ModelRanking保存top-5最佳模型
- ✅ 自动删除性能较差的模型

---

### 6.2 模型管理

#### 旧版本（HPPO）
```python
# 无模型管理机制
# 所有模型都保存，可能占用大量空间
```

#### 新版本（独立PPO）
```python
# PPO_episoid_1.py (新)
class ModelRanking:
    def __init__(self, top_n=5):
        self.top_n = top_n
        self.rankings = []  # 最小堆
    
    def add_and_manage(self, new_score, new_checkpoint, episode_id, base_dir):
        # 如果排行榜未满，直接保存
        if len(self.rankings) < self.top_n:
            should_save = True
        # 如果新模型比最差的要好，替换
        elif new_score > self.rankings[0][0]:
            should_save = True
            worst_score, worst_path = heapq.heappop(self.rankings)
            os.remove(worst_path)  # 删除最差模型
```

**特点**：
- ✅ 只保存top-5最佳模型
- ✅ 自动删除性能较差的模型
- ✅ 节省存储空间

---

## 七、日志记录差异

### 7.1 日志字段

#### 旧版本（HPPO）
```python
# PPO_episoid_1.py (旧)
log_writer_catch.add_action_catch(action_shoulder, action_arm)
log_writer_catch.add_log_prob_catch(log_prob_shoulder, log_prob_arm)
log_writer_catch.add_value_catch(value, value)  # 单一value
log_writer_catch.add_loss_hppo_catch(loss1, loss2)  # 离散和连续损失
```

**特点**：
- ✅ 记录离散和连续损失
- ✅ 单一value（共享Critic）

#### 新版本（独立PPO）
```python
# PPO_episoid_1.py (新)
log_writer_catch.add_action_catch(action_shoulder, action_arm)
log_writer_catch.add_log_prob_catch(log_prob_shoulder, log_prob_arm)
log_writer_catch.add_value_catch(value_shoulder, value_arm)  # 两个独立value
log_writer_catch.add(loss=loss)  # 总损失（shoulder + arm）
log_writer_catch.add(success_rate=test_success_rate)  # 测试成功率
log_writer_catch.add(sigma=current_sigma)  # 探索参数
```

**特点**：
- ✅ 记录两个独立的value
- ✅ 记录总损失
- ✅ 记录测试成功率
- ✅ 记录sigma（探索参数）

---

## 八、环境初始化差异

### 8.1 传感器稳定性检查

#### 旧版本（HPPO）
```python
# PPO_episoid_1.py (旧)
env.reset()
env.wait(500)  # 简单等待
# 没有传感器稳定性检查
```

#### 新版本（独立PPO）
```python
# PPO_episoid_1.py (新)
env.reset()
env.wait(500)
# 使用工具函数检查传感器状态
if not wait_for_sensors_stable(env, max_retries=40, wait_ms=200):
    print("警告: 传感器不稳定，尝试重置环境...")
    reset_environment(env)
```

**特点**：
- ✅ 检查传感器稳定性
- ✅ 如果传感器不稳定，自动重置
- ✅ 提高训练稳定性

---

## 九、关键代码逻辑差异总结

### 9.1 动作选择流程

| 步骤 | 旧版本（HPPO） | 新版本（独立PPO） |
|------|--------------|------------------|
| 1. 网络前向传播 | 单一网络输出离散+连续 | 两个独立网络各自输出 |
| 2. 动作采样 | 离散采样 + 连续采样 | 每个网络独立采样 |
| 3. 动作处理 | 根据离散动作掩码 | 直接使用，无需掩码 |
| 4. 动作执行 | 执行掩码后的动作 | 执行原始动作（限幅） |

### 9.2 学习更新流程

| 步骤 | 旧版本（HPPO） | 新版本（独立PPO） |
|------|--------------|------------------|
| 1. 数据存储 | 条件存储（good=1） | 无条件存储所有步 |
| 2. 优势计算 | 单一优势函数 | 每个智能体独立计算 |
| 3. 损失计算 | 离散损失 + 连续损失 | 每个智能体独立损失 |
| 4. 网络更新 | 三个优化器分别更新 | 每个智能体独立更新 |
| 5. 更新轮数 | 10轮 | 4轮 |

### 9.3 模型管理流程

| 步骤 | 旧版本（HPPO） | 新版本（独立PPO） |
|------|--------------|------------------|
| 1. 模型保存 | goal=1或每100步 | 每500步测试后 |
| 2. 模型评估 | 无 | 100轮测试计算成功率 |
| 3. 模型管理 | 无，所有模型都保存 | top-5最佳模型 |
| 4. 模型加载 | 加载最新模型 | 加载最新模型 |

---

## 十、性能影响分析

### 10.1 计算效率

**旧版本（HPPO）**：
- ✅ 单一网络，前向传播一次
- ❌ 三个优化器，更新开销大
- ❌ 更新轮数多（10轮），训练慢

**新版本（独立PPO）**：
- ❌ 两个网络，前向传播两次
- ✅ 单一优化器，更新开销小
- ✅ 更新轮数少（4轮），训练快

### 10.2 内存占用

**旧版本（HPPO）**：
- ✅ 单一网络，内存占用小
- ❌ 三个优化器状态，内存占用中等

**新版本（独立PPO）**：
- ❌ 两个网络，内存占用大
- ✅ 两个优化器状态，内存占用中等
- ✅ 完整的GNN结构，内存占用大

### 10.3 训练稳定性

**旧版本（HPPO）**：
- ❌ 条件存储（good=1），可能丢失数据
- ❌ 没有测试机制，无法评估性能
- ❌ 没有传感器稳定性检查

**新版本（独立PPO）**：
- ✅ 无条件存储，符合on-policy
- ✅ 定期测试，评估性能
- ✅ 传感器稳定性检查，提高稳定性

---

## 十一、关键差异总结

### 11.1 架构层面

1. **网络结构**：
   - 旧：单一混合网络（离散+连续）
   - 新：两个独立网络（shoulder + arm）

2. **图神经网络**：
   - 旧：简化为全连接层
   - 新：完整的5层GNN结构

3. **动作空间**：
   - 旧：混合动作空间（离散开关 + 连续值）
   - 新：纯连续动作空间

### 11.2 训练层面

1. **数据存储**：
   - 旧：条件存储（good=1）
   - 新：无条件存储所有步

2. **更新机制**：
   - 旧：三个优化器分别更新（10轮）
   - 新：单一优化器更新（4轮）

3. **探索策略**：
   - 旧：entropy_coef=0.01（保守）
   - 新：entropy_coef=0.1（积极）

### 11.3 评估层面

1. **测试机制**：
   - 旧：无测试机制
   - 新：每500步测试100轮

2. **模型管理**：
   - 旧：所有模型都保存
   - 新：只保存top-5最佳模型

3. **传感器检查**：
   - 旧：无检查
   - 新：稳定性检查

---

## 十二、建议

### 12.1 如果使用HPPO（旧版本）

**优点**：
- ✅ 单一网络，计算效率高
- ✅ 动作掩码机制，可以控制动作更新频率

**改进建议**：
1. 添加测试机制，定期评估性能
2. 添加传感器稳定性检查
3. 考虑无条件存储数据（符合on-policy）
4. 减少更新轮数（10轮可能过多）
5. 增加entropy_coef，鼓励探索

### 12.2 如果使用独立PPO（新版本）

**优点**：
- ✅ 符标准PPO实现
- ✅ 有完整的测试和评估机制
- ✅ 传感器合稳定性检查

**改进建议**：
1. 考虑减少测试频率（500步可能过长）
2. 考虑减少测试轮数（100轮可能过多）
3. 可以考虑使用共享特征层（减少内存占用）

---

**最后更新**：基于代码对比分析  
**关键结论**：除了多智能体和奖励机制，主要差异在于网络架构、训练机制、评估机制和模型管理

